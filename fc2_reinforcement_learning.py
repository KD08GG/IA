# -*- coding: utf-8 -*-
"""FC2: Reinforcement Learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/161edFHtZpnxlttxCj31cakcXUOA--bdG

## Implementaci칩n del Entorno del Almac칠n
"""

def random_obstacle() :
  import random
  x = random.randint(0, 9)
  #print(x)
  y = random.randint(0, 9)
  #print(y)
  if (x == 2 and y == 2 )or (x == 8 and y == 8) or (x==3 and y==3) or (x==5 and y==5) or (x== 1 and y== 1):
    random_obstacle()
  else:
    return (x,y)

import gym
import numpy as np
from gym import spaces

class WarehouseEnv(gym.Env):
    def __init__(self, grid_size=10):
        super(WarehouseEnv, self).__init__()
        self.delivered_packages = 0  # Contador de paquetes entregados


        # Tama침o de la cuadr칤cula
        self.grid_size = grid_size

        #Random obstacle
        x_r, y_r = random_obstacle()
        print("Random obstacle generated at [", x_r,",", y_r, "]")

        # Configuraci칩n del almac칠n
        self.obstacles = {(3, 3), (5,5), (x_r,y_r)}
        self.pickup_point = (9, 4)
        self.delivery_point = (8, 8)

        # Espacios de acci칩n y observaci칩n
        self.action_space = spaces.Discrete(6)  # 0-3 (mover), 4 (recoger), 5 (entregar)
        self.observation_space = spaces.Box(
            low=np.array([0, 0, 0, 0]),
            high=np.array([grid_size - 1, grid_size - 1, 1, 100]),
            dtype=np.int32
        )

        # Estado inicial
        self.reset()

    def reset(self):
        """Reinicia el entorno y devuelve el estado inicial."""
        self.robot_pos = [1, 1]  # Posici칩n inicial del robot
        self.has_item = 0        # Estado del 칤tem (0 = no tiene, 1 = tiene)
        self.energy = 100         # Energ칤a inicial
        #x_r, y_r = random_obstacle()
        #print("Reset Random obstacle generated at [", x_r,",", y_r, "]")
        return np.array(self.robot_pos + [self.has_item, self.energy])

    def step(self, action):
        """Ejecuta una acci칩n y devuelve (estado, recompensa, done, info)."""

        # Si la energ칤a est치 en 0, termina el episodio autom치ticamente
        if self.energy == 0:
            return np.array(self.robot_pos + [self.has_item, self.energy]), -5, True, {}

        # Inicializar recompensa y done
        reward = -1  # Penalizaci칩n base por movimiento
        done = False

        # Guardar posici칩n previa
        prev_pos = self.robot_pos.copy()

        # Desglosar la acci칩n
        if action in {0, 1, 2, 3}:  # Acciones de movimiento
            self.move(action)

        # Si el robot choca con un obst치culo, no se mueve y recibe penalizaci칩n
        if tuple(self.robot_pos) in self.obstacles:
            self.robot_pos = prev_pos  # Regresa a la posici칩n anterior
            reward = -10  # Penalizaci칩n por colisi칩n

        elif action == 4:  # Recoger 칤tem
            if tuple(self.robot_pos) == self.pickup_point and self.has_item == 0:
                self.has_item = 1
                #reward = 5  # Bonificaci칩n por recoger

        elif action == 5:  # Entregar 칤tem
            if tuple(self.robot_pos) == self.delivery_point and self.has_item == 1:
                reward = 10
                self.has_item = 0  # Suelta el paquete, pero sigue jugando
                self.delivered_packages += 1  # Incrementar el contador

        # Reducir energ칤a solo si no ha terminado el episodio
        self.energy -= 1
        if self.energy == 0:  # Si la energ칤a llega a 0, termina el episodio
            reward = -5
            done = True

        # Retornar estado, recompensa, done, info
        return np.array(self.robot_pos + [self.has_item, self.energy]), reward, done, {}

    def move(self, action):
        """Mueve al robot en la direcci칩n dada (0 = arriba, 1 = abajo, 2 = izquierda, 3 = derecha)."""
        x, y = self.robot_pos

        if action == 0:  # Arriba
            y = max(0, y - 1)
        elif action == 1:  # Abajo
            y = min(self.grid_size - 1, y + 1)
        elif action == 2:  # Izquierda
            x = max(0, x - 1)
        elif action == 3:  # Derecha
            x = min(self.grid_size - 1, x + 1)

        self.robot_pos = [x, y]  # Mueve al robot

"""## Implementaci칩n del Agente de Aprendizaje por Refuerzo"""

import random
import numpy as np
from collections import defaultdict

class QLearningAgent:
    def __init__(self, env, alpha=0.99, gamma=0.8, epsilon=1.0, epsilon_decay=0.98, epsilon_min=0.01):
        self.env = env
        self.q_table = defaultdict(lambda: np.zeros(env.action_space.n))
        self.alpha = alpha  # Tasa de aprendizaje
        self.gamma = gamma  # Factor de descuento
        self.epsilon = epsilon  # Probabilidad inicial de exploraci칩n
        self.epsilon_decay = epsilon_decay  # Factor de reducci칩n de exploraci칩n
        self.epsilon_min = epsilon_min  # Valor m칤nimo de epsilon

    def choose_action(self, state):
        if random.uniform(0, 1) < self.epsilon:
            return self.env.action_space.sample()  # Exploraci칩n
        else:
            return np.argmax(self.q_table[tuple(state)])  # Explotaci칩n

    def update_q_table(self, state, action, reward, next_state):
        old_value = self.q_table[tuple(state)][action]
        next_max = np.max(self.q_table[tuple(next_state)])
        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)
        self.q_table[tuple(state)][action] = new_value

    def update_epsilon(self):
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

"""## Entrenamiento del Agente"""

import numpy as np
import matplotlib.pyplot as plt
import time

def render(self, ax, img):
    """Renderiza la cuadr칤cula y actualiza en tiempo real."""
    grid = np.zeros((self.grid_size, self.grid_size))  # Matriz vac칤a

    for obs in self.obstacles:
        grid[obs] = 1  # Obst치culos
    grid[self.pickup_point] = 2  # Punto de recogida
    grid[self.delivery_point] = 3  # Punto de entrega

    x, y = self.robot_pos
    grid[y, x] = 4 if self.has_item else 5  # Robot (con o sin paquete)

    img.set_data(grid)  # 游댠 Actualiza la cuadr칤cula sin crear una nueva
    plt.draw()
    plt.pause(0.3)  # Peque침a pausa para que se note el movimiento

env = WarehouseEnv()
agent = QLearningAgent(env)

episodes = 10000
rewards_per_episode = []
entregas_historial = []
trayectoria_ultimo_ep = [(1, 1)]  # Aqu칤 guardaremos la trayectoria del 칰ltimo episodio

for episode in range(episodes):
    state = env.reset()
    total_reward = 100  # Esto depende de tu configuraci칩n
    done = False
    env.delivered_packages = 0  # Reiniciar el contador de paquetes
    trayectoria_actual = [(1, 1)]  # Guardar trayectoria del episodio actual
    #trayectoria_ultimo_ep = [(1, 1)]

    while not done:
        action = agent.choose_action(state)
        next_state, reward, done, _ = env.step(action)

        # Guardar la posici칩n actual del robot
        trayectoria_actual.append(tuple(env.robot_pos))

        agent.update_q_table(state, action, reward, next_state)
        state = next_state
        total_reward += reward
        agent.update_epsilon()

    # Guardar la trayectoria del 칰ltimo episodio
    if episode == episodes - 1:
        trayectoria_ultimo_ep = trayectoria_actual

    entregas_historial.append(env.delivered_packages)
    rewards_per_episode.append(total_reward)

# Guardar la trayectoria dentro del objeto `env`
env.trayectoria_ultimo_ep = trayectoria_ultimo_ep

print("Entrenamiento completado.")

"""## Evaluaci칩n y Comparaci칩n con un Algoritmo Heur칤stico"""

import matplotlib.pyplot as plt

plt.plot(rewards_per_episode)
plt.xlabel("Episodes")
plt.ylabel("Total Rewards")
plt.title("RL Agent Learning")
plt.show()

import matplotlib.pyplot as plt

# Asumiendo que tienes una lista 'rewards_per_episode' con recompensas de 10,000 episodios
# Seleccionamos los 칰ltimos 1000 episodios
last_1000_rewards = rewards_per_episode[-100:]

plt.plot(last_1000_rewards)
plt.xlabel("Episodes (last 100)")
plt.ylabel("Total Reward")
plt.title("RL Agent Learning")
plt.show()

import matplotlib.pyplot as plt

plt.plot(entregas_historial)
plt.xlabel("Episodes")
plt.ylabel("Packages deliveres")
plt.title("Evolution of Packages delivered")
plt.grid(True)
plt.show()

# Calcular el promedio de puntos por episodio
promedio_puntos = sum(rewards_per_episode) / len(rewards_per_episode)

# Calcular el promedio de paquetes entregados por episodio
promedio_entregas = sum(entregas_historial) / len(entregas_historial)

# Mostrar los resultados
print(f"Promedio de puntos por episodio: {promedio_puntos:.2f}")
print(f"Promedio de paquetes entregados por episodio: {promedio_entregas:.2f}")

"""## Visualizaci칩n con Animation"""

print( trayectoria_ultimo_ep)

import matplotlib.pyplot as plt
import matplotlib.animation as animation
import numpy as np
from IPython.display import HTML  # Importante para mostrar la animaci칩n en Colab

# Verificar que la trayectoria se haya registrado correctamente
if not hasattr(env, "trayectoria_ultimo_ep") or len(env.trayectoria_ultimo_ep) == 0:
    raise ValueError("No se ha registrado la trayectoria del 칰ltimo episodio.")

trayectoria_ultimo_ep = env.trayectoria_ultimo_ep  # Lista de posiciones [(x, y)]
grid_size = env.grid_size  # Tama침o del grid

# Crear la figura y los ejes
fig, ax = plt.subplots(figsize=(6, 6))  # Aumentar el tama침o para mayor claridad
ax.set_xlim(-0.5, grid_size - 0.5)
ax.set_ylim(grid_size - 0.5, -0.5)  # Se invierte el eje Y para que la cuadr칤cula se vea correctamente

# Mejorar la cuadr칤cula
ax.set_xticks(np.arange(0, grid_size, 1))
ax.set_yticks(np.arange(0, grid_size, 1))
ax.set_xticklabels(np.arange(0, grid_size, 1), fontsize=10, color='black')
ax.set_yticklabels(np.arange(0, grid_size, 1), fontsize=10, color='black')
ax.grid(which='both', color='gray', linestyle='-', linewidth=0.5)

# Dibujar obst치culos
for obs in env.obstacles:
    ax.add_patch(plt.Rectangle((obs[0] - 0.5, obs[1] - 0.5), 1, 1, color="black"))

# Dibujar el punto de recogida
pickup_x, pickup_y = env.pickup_point
ax.add_patch(plt.Rectangle((pickup_x - 0.5, pickup_y - 0.5), 1, 1, color="blue", alpha=0.6))

# Dibujar el punto de entrega
delivery_x, delivery_y = env.delivery_point
ax.add_patch(plt.Rectangle((delivery_x - 0.5, delivery_y - 0.5), 1, 1, color="magenta", alpha=1))

# Inicializar el robot en su primera posici칩n
robot, = ax.plot([], [], "ro", markersize=10)  # "ro" = red circle (robot)

# Funci칩n para inicializar la animaci칩n
def init():
    robot.set_data([], [])
    return robot,

# Funci칩n para actualizar la animaci칩n
def update(frame):
    # Obtener las coordenadas del robot en el frame actual
    x, y = trayectoria_ultimo_ep[frame]

    # Imprimir las coordenadas de cada paso para verificar
    #print(f"Frame {frame}: Robot en ({x}, {y})")

    # Actualizar la posici칩n del robot en la gr치fica
    robot.set_data([x], [y])

    return robot,  # Devolver el objeto que se actualiza

# Crear la animaci칩n con intervalos de 1 segundo entre cada posici칩n
ani = animation.FuncAnimation(fig, update, frames=len(trayectoria_ultimo_ep), init_func=init, interval=1000, repeat=False)

# Mostrar la animaci칩n en Google Colab
HTML(ani.to_jshtml())  # Esto permite que la animaci칩n se muestre en Google Colab